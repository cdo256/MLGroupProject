{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T23:23:07.196834500Z",
     "start_time": "2024-11-05T23:23:06.630572900Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23120\\AppData\\Local\\Temp\\ipykernel_45668\\3447305422.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_df[col] = num_df[col].replace(-1, pd.NA)\n",
      "C:\\Users\\23120\\AppData\\Local\\Temp\\ipykernel_45668\\3447305422.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_df[col] = num_df[col].fillna(base_df[col].mean())\n"
     ]
    }
   ],
   "source": [
    "%run BankPreprocess.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T23:28:20.089751200Z",
     "start_time": "2024-11-05T23:28:19.382263400Z"
    }
   },
   "id": "466ee0217862f95e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      " #   Column                   Non-Null Count  Dtype\n",
      "---  ------                   --------------  -----\n",
      " 0   age                      45211 non-null  int64\n",
      " 1   job                      45211 non-null  int64\n",
      " 2   marital                  45211 non-null  int64\n",
      " 3   education                45211 non-null  int64\n",
      " 4   has_defaulted            45211 non-null  int64\n",
      " 5   balance                  45211 non-null  int64\n",
      " 6   housing                  45211 non-null  int64\n",
      " 7   loan                     45211 non-null  int64\n",
      " 8   contact                  45211 non-null  int64\n",
      " 9   day                      45211 non-null  int64\n",
      " 10  month                    45211 non-null  int64\n",
      " 11  duration                 45211 non-null  int64\n",
      " 12  campaign                 45211 non-null  int64\n",
      " 13  days_since_last_contact  45211 non-null  int64\n",
      " 14  prev_contact_count       45211 non-null  int64\n",
      " 15  prev_outcome             45211 non-null  int64\n",
      " 16  outcome                  45211 non-null  int64\n",
      "dtypes: int64(17)\n",
      "memory usage: 5.9 MB\n"
     ]
    }
   ],
   "source": [
    "ord_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T23:30:07.007993700Z",
     "start_time": "2024-11-05T23:30:06.953402600Z"
    }
   },
   "id": "f4e7b50b2678af26",
   "execution_count": 4
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocess\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = ord_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#  Reduce Dimensionality by getting top x corelated features\n",
    "# we should use decision tree to rank features, better correlated features doesn't mean better outcome\n",
    "# random forest is able to do feature interactions, check how different features interact with each other\n",
    "# with random forest we may not need pca to reduce dimensionality because random forest can handle the feature selection\n",
    "# may not want to reduce feature using pca, also loose interpretability\n",
    "# Get the absolute values of the correlation matrix\n",
    "abs_corr = correlation_matrix.abs()\n",
    "\n",
    "# Unstack the matrix to get a Series\n",
    "corr_pairs = abs_corr.unstack()\n",
    "\n",
    "# Drop self-correlations (where the index and columns are the same)\n",
    "corr_pairs = corr_pairs[corr_pairs < 1]\n",
    "\n",
    "# Sort the pairs by correlation values in descending order\n",
    "sorted_pairs = corr_pairs.sort_values(ascending=False)\n",
    "\n",
    "# Get the top 5 most strongly correlated pairs\n",
    "top_10 = sorted_pairs.head(10)\n",
    "\n",
    "print(\"Top 10 Most Strongly Correlated Features:\")\n",
    "print(top_10)"
   ],
   "id": "e6a3e8bbd58ce1e"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Extract unique columns from the top 5 pairs\n",
    "top_10_columns = set()\n",
    "for index in top_10.index:\n",
    "    top_10_columns.update(index)\n",
    "\n",
    "# Convert set to list\n",
    "top_10_columns = list(top_10_columns)\n",
    "\n",
    "# Drop all columns not in top 10\n",
    "new_df = ord_df[top_10_columns]\n",
    "new_df = pd.concat([new_df, ord_df['balance']],axis=1)\n",
    "\n",
    "# Iterate through each numerical column to check for outliers and normalize\n",
    "for col in new_df.select_dtypes(include='number').columns:\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = new_df[col].quantile(0.25)\n",
    "    Q3 = new_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Determine outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove outliers by clipping\n",
    "    new_df[col] = new_df[col].clip(lower_bound, upper_bound)\n",
    "\n",
    "    # Calculate the 5th and 95th percentiles for normalization\n",
    "    p5 = new_df[col].quantile(0.05)\n",
    "    p95 = new_df[col].quantile(0.95)\n",
    "\n",
    "    # Normalize the data\n",
    "    new_df[col] = (new_df[col] - p5) / (p95 - p5)\n",
    "\n",
    "    # Clip normalized values to [0, 1]\n",
    "    new_df[col] = new_df[col].clip(0, 1)\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(1,figsize=(20,8))\n",
    "sns.boxplot(data=new_df,ax=ax)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "2b7adc0cc6b05f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hindsight trying to predict age from the financial data, marriage, duration and balance is not meaningful, however model performs better predicting age than balance on this dataset. We are able to visualise NA Values as absent in boxplot above so we can drop the NA features. We could add a method to allow mix max normalisation to retain na features but this isn't a priority for this assessment. \n",
    "\n",
    "# Assign the 'balance' column to y\n",
    "y = new_df['age']\n",
    "\n",
    "# Drop the 'balance' column and assign the result to X\n",
    "X = new_df.drop(['days_since_last_contact','age', 'prev_contact_count', 'prev_outcome', 'outcome'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T23:37:50.413197100Z",
     "start_time": "2024-11-05T23:37:50.357869600Z"
    }
   },
   "id": "3d7a82eb0052257",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   marital  duration   balance\n0      1.0  0.371711  0.637039\n1      0.0  0.190789  0.055311\n2      1.0  0.067434  0.047881\n3      1.0  0.093750  0.461750\n4      0.0  0.268092  0.047606",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>marital</th>\n      <th>duration</th>\n      <th>balance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.371711</td>\n      <td>0.637039</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.190789</td>\n      <td>0.055311</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.067434</td>\n      <td>0.047881</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.093750</td>\n      <td>0.461750</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.268092</td>\n      <td>0.047606</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.info()\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T23:37:59.318130900Z",
     "start_time": "2024-11-05T23:37:59.210076300Z"
    }
   },
   "id": "d01fea516af2d571",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA, we are turning 3 features into 2 features to reduce the dimensionality. PCA can be applied here as we don't need to preserve original structure of data, interpretability of features is not important and features have the same scales.\n",
    "pca = PCA(n_components=2)  # Retain the first 2 components\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:39:54.982317100Z",
     "start_time": "2024-11-05T23:39:54.931809200Z"
    }
   },
   "id": "5439f50c02defc9c",
   "execution_count": 24
  },
  {
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-11-05T23:40:04.033143100Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set up the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 0.5],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = SVR(C=0.1,epsilon=0.1,kernel=\"rbf\")\n",
    "svm.fit(X_train_pca, y_train)\n",
    "# Calculate MSE\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Best parameters from grid search\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ],
   "id": "2bc277d20897531f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Initialize the SVM model\u001B[39;00m\n\u001B[0;32m      9\u001B[0m svm \u001B[38;5;241m=\u001B[39m SVR(C\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,epsilon\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrbf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m svm\u001B[38;5;241m.\u001B[39mfit(\u001B[43mX_train_pca\u001B[49m, y_train)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Calculate MSE\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Set up GridSearchCV\u001B[39;00m\n\u001B[0;32m     13\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(estimator\u001B[38;5;241m=\u001B[39msvm, param_grid\u001B[38;5;241m=\u001B[39mparam_grid, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneg_mean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train_pca' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the best model to make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(best_model.score(X_test_pca, y_test))\n",
    "\n",
    "# We could also look into using ROC Curves to visualise the different results of models and compare during kfold. "
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:35:19.471776300Z",
     "start_time": "2024-11-05T23:35:19.422151100Z"
    }
   },
   "id": "3908645649876d02",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a4048e1f9126265c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
