{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocess\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = ord_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#  Reduce Dimensionality by getting top x corelated features\n",
    "# we should use decision tree to rank features, better correlated features doesn't mean better outcome\n",
    "# random forest is able to do feature interactions, check how different features interact with each other\n",
    "# with random forest we may not need pca to reduce dimensionality because random forest can handle the feature selection\n",
    "# may not want to reduce feature using pca, also loose interpretability\n",
    "# Get the absolute values of the correlation matrix\n",
    "abs_corr = correlation_matrix.abs()\n",
    "\n",
    "# Unstack the matrix to get a Series\n",
    "corr_pairs = abs_corr.unstack()\n",
    "\n",
    "# Drop self-correlations (where the index and columns are the same)\n",
    "corr_pairs = corr_pairs[corr_pairs < 1]\n",
    "\n",
    "# Sort the pairs by correlation values in descending order\n",
    "sorted_pairs = corr_pairs.sort_values(ascending=False)\n",
    "\n",
    "# Get the top 5 most strongly correlated pairs\n",
    "top_10 = sorted_pairs.head(10)\n",
    "\n",
    "print(\"Top 10 Most Strongly Correlated Features:\")\n",
    "print(top_10)\n",
    "\n",
    "# Extract unique columns from the top 5 pairs\n",
    "top_10_columns = set()\n",
    "for index in top_10.index:\n",
    "    top_10_columns.update(index)\n",
    "\n",
    "# Convert set to list\n",
    "top_10_columns = list(top_10_columns)\n",
    "\n",
    "# Drop all columns not in top 10\n",
    "new_df = ord_df[top_10_columns]\n",
    "new_df = pd.concat([new_df, ord_df['balance']],axis=1)\n",
    "\n",
    "# Iterate through each numerical column to check for outliers and normalize\n",
    "for col in new_df.select_dtypes(include='number').columns:\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = new_df[col].quantile(0.25)\n",
    "    Q3 = new_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Determine outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove outliers by clipping\n",
    "    new_df[col] = new_df[col].clip(lower_bound, upper_bound)\n",
    "\n",
    "    # Calculate the 5th and 95th percentiles for normalization\n",
    "    p5 = new_df[col].quantile(0.05)\n",
    "    p95 = new_df[col].quantile(0.95)\n",
    "\n",
    "    # Normalize the data\n",
    "    new_df[col] = (new_df[col] - p5) / (p95 - p5)\n",
    "\n",
    "    # Clip normalized values to [0, 1]\n",
    "    new_df[col] = new_df[col].clip(0, 1)\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(1,figsize=(20,8))\n",
    "sns.boxplot(data=new_df,ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Assign the 'balance' column to y\n",
    "y = new_df['age']\n",
    "\n",
    "# Drop the 'balance' column and assign the result to X\n",
    "X = new_df.drop(['age', 'prev_contact_count', 'prev_outcome', 'outcome'], axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=5)  # Retain the first 5 components\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ],
   "id": "5439f50c02defc9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set up the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 0.5],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = SVR(C=0.1,epsilon=0.1,kernel=\"rbf\")\n",
    "svm.fit(X_train_pca, y_train)\n",
    "# Calculate MSE\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Best parameters from grid search\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(best_model.score(predictions, y_test))"
   ],
   "id": "3908645649876d02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
